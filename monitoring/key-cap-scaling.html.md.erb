---
title: Key Capacity Scaling Indicators
owner: PCF Metrics Platform Monitoring
---
This topic describes key capacity scaling indicators that operators monitor to determine when they need to scale their
<%= vars.app_runtime_full %> (<%= vars.app_runtime_abbr %>) deployments.


## <a id='overview'></a> Overview

<%= vars.company_name %> provides these indicators to operators as general guidance for capacity scaling. Each indicator is based on all platform metrics from all components. <br><br>This guidance is applicable to most <%= vars.app_runtime_abbr %> deployments. <%= vars.company_name %> recommends that operators fine-tune the suggested alert thresholds by observing historical trends for their deployments.

For more information about accessing metrics used in these key capacity scaling indicators, see <a href="https://docs.pivotal.io/application-service/2-11/loggregator/data-sources.html">Overview of Logging and Metrics</a>.

## <a id="cell"></a> Diego Cell Capacity Scaling Indicators

There are three key capacity scaling indicators <%= vars.company_name %> recommends for a Diego Cell.

<table>
  <tr><th id="cell-memory" colspan="2" style="text-align: center;"><br>Diego Cell Memory Capacity<br><br></th></tr>
    <tr>
      <th width="25%">Description</th>
        <td>
          The Diego Cell Memory Capacity indicator is the percentage of remaining memory your Diego Cells can allocate to containers.<br>
          Divide the <code>CapacityRemainingMemory</code> metric with the <code>CapacityTotalMemory</code> to get this percentage.<br>
          The metric <code>CapacityRemainingMemory</code> is the remaining memory, in MiB, available to a Diego Cell.<br>
          The metric <code>CapacityTotalMemory</code> is the total memory, in MiB, available to a Diego Cell.<br>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>rep</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>CapacityRemainingMemory</code><br>
          <code>CapacityTotalMemory</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>&lt; average (35%)<br>
          This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Deploy additional diego cells until the average free memory is 35%. This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (%)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:diego_cells
        </td>
    </tr>
</table>

<table>
   <tr><th id="cell-disk" colspan="2" style="text-align: center;"><br>Diego Cell Disk Capacity<br><br></th></tr>
    <tr>
      <th width="25%">Description</th>
        <td>
          The Diego Cell Disk Capacity indicator is the percentage of remaining disk capacity a given Diego Cell can allocate to containers.<br>
          Divide the <code>CapacityRemainingDisk</code> metric by the <code>CapacityTotalDisk</code> metric to get this percentage. <br>
          The metric <code>CapacityRemainingDisk</code> is the remaining amount of disk avaiable, in MiB, for this Diego Cell.<br>
          The metric <code>CapacityTotalDisk</code> indicates the total amount of disk available, in MiB, for this Diego Cell.<br>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>rep</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>CapacityRemainingDisk</code><br>
          <code>CapacityTotalDisk</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>&lt; average (35%)<br>
          This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Deploy additional diego cells until the average free memory is 35%. This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (%)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:diego_cells
        </td>
    </tr>
</table>

<table>
  <tr><th id="cell-container" colspan="2" style="text-align: center;"><br>Diego Cell Container Capacity<br><br></th></tr>
    <tr>
      <th width="25%">Description</th>
        <td>
          The Diego Cell Container Capacity indicator is the percentage of containers remaining that a given Diego Cell can host.<br>
          Divide the <code>CapacityRemainingContainers</code> metric by the <code>CapacityTotalContainers</code> metric to get this percentage.<br>
          The metric <code>CapacityRemainingContainers</code> is the remaining number of containers.<br>
          The metric <code>CapacityTotalContainer</code> is the total number of containers.<br>
        </td>
    <tr>
      <th>Source ID</th>
        <td>
          <code>rep</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>CapacityRemainingContainers</code><br>
          <code>CapacityTotalContainers</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>&lt; average (35%)<br>
        This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Deploy additional diego cells until the average free memory is 35%. This threshold assumes you have three AZs.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (%)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:diego_cells
        </td>
    </tr>
</table>


## <a id="doppler-server"></a> Firehose Performance Scaling Indicators

<%= vars.company_name %> recommends three key capacity scaling indicators for monitoring Firehose performance.

<table>
  <tr><th id="firehose-loss-rate" id="firehose-loss-rate" colspan="2" style="text-align: center;"><br>Log Transport Loss Rate<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>
          The Log Transport Loss Rate indicator is the rate of messages dropped between the Dopplers and the Firehouse.<br>
          Divide the <code>dropped{direction=ingress}</code> metric by the <code>ingress</code> metric to get the loss rate.<br><br>
          Metric <code>ingress</code> is the number of messages entering the Dopplers. <code>dropped</code> is the number of messages never delivered to the Firehose.<br><br>
          For more information about Loggregator components, see <a href="../../loggregator/architecture.html">Loggregator Architecture</a>.<br>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>doppler</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>dropped</code><br>
          <code>ingress</code>
        </td>
    </tr>
    <tr>
      <th>Label</th>
        <td>
          <code>{direction=ingress}</code><br>
          Dopplers emit two separate dropped metrics, one for ingress and one for egress. The envelopes have a <code>direction</code> label. For this indicator, use the metric with a <code>direction</code> tag with a value of <code>ingress</code>.
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>
          <strong>Scale indicator:</strong> &ge; 0.01</br>
          If alerting:<br>
          <strong>Yellow warning:</strong> &ge; 0.005<br>
          <strong>Red critical:</strong> &ge; 0.01 <br>
          Excessive dropped messages can indicate the Dopplers or Traffic Controllers are not processing messages quickly enough.
        </td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>
          Scale up the number of Traffic Controller and Doppler instances.
          <p class="note"><strong>Note:</strong> At approximately 40 Doppler instances and 20 Traffic Controller instances, horizontal scaling is no longer useful for improving Firehose performance. To improve performance, add vertical scale to the existing Doppler and Traffic Controller instances by increasing CPU resources.</p>
        </td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (float)<br>
          <strong>Frequency:</strong> Base metrics are emitted every 5&nbsp;s<br>
          <strong>Applies to:</strong> cf:doppler<br>
        </td>
    </tr>
</table>

<table>
  <tr><th id="doppler-message-rate-ksi" colspan="2" style="text-align: center;">Doppler Message Rate Capacity</th></tr>
    <tr>
      <th width="25">Description</th>
        <td>The Doppler Message Rate Capacity indicator is the average number of messages per Doppler instance. Divide the sum of <code>ingress</code> metrics across instances by the <code>current number of Doppler instances</code> to get this
        average.</td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>doppler</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>ingress</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td><strong>Scale indicator:</strong> &ge; 16,000 envelopes per second (or 1 million envelopes per minute)</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Increase the number of Doppler VMs in the <strong>Resource Config</strong> pane of the <%= vars.app_runtime_abbr %> tile.</td>
     </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (float)<br>
          <strong>Frequency:</strong> Emitted every 5 s<br>
          <strong>Applies to:</strong> cf:doppler<br>
        </td>
    </tr>
</table>

<table>
  <tr><th id="rlp-ksi" colspan="2" style="text-align: center;"><br>Reverse Log Proxy Loss Rate<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>
          The Reverse Log Proxy Loss Rate indicator is the rate of bound app logs dropped from the Reverse Log Proxies (RLP). Divide the <code>dropped</code> metric by the <code>ingress</code> metric to get this indicator.<br><br>
          This loss rate is specific to the RLP and does not impact the Firehose loss rate.<br>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>rlp</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>ingress</code><br>
          <code>dropped</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>
          <strong>Scale indicator:</strong> &ge; 0.1</br>
          If alerting:<br>
          <strong>Yellow warning:</strong> &ge; 0.01<br>
          <strong>Red critical:</strong> &ge; 0.1 <br>
          Excessive dropped messages can indicate that the RLP is overloaded and that the Traffic Controllers need to be scaled.
        </td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale up the number of traffic controller instances to further balance log load.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Counter (Integer)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:loggregator<br>
        </td>
    </tr>
</table>


## <a id="firehose-consumer"></a> Firehose Consumer Scaling Indicator

<%= vars.company_name %> recommends the following scaling indicator for monitoring the performance of consumers of the Firehose.

<table>
  <tr><th id="slow-consumer" colspan="2" style="text-align: center;"><br>Slow Consumer Drops<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>The Slow Consumer Drops indicator is the <code>slow_consumer</code> metric incremented for each connection the Firehose closes because a consumer could not keep up.<br>
          This indicator shows how fast a Firehose consumer, such as a monitoring tool nozzle, is ingesting data. If this number is anomalous, it may result in the downstream monitoring tool not having all expected data, even though that data was successfully transported through the Firehose.</td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>doppler_proxy</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>slow_consumer</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td><strong>Scale indicator:</strong> <%= vars.company_name %> recommends scaling when the rate of Firehose Slow Consumer Drops is anomalous for a given environment.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale up the number of nozzle instances. You can scale a nozzle using the subscription ID specified when the nozzle connects to the Firehose. If you use the same subscription ID on each nozzle instance, the Firehose evenly distributes data across all instances of the nozzle. For example, if you have two nozzle instances with the same subscription ID, the Firehose sends half of the data to one nozzle instance and half to the other. Similarly, if you have three nozzle instances with the same subscription ID, the Firehose sends one-third of the data to each instance. If you want to scale a nozzle, the number of nozzle instances should match the number of Traffic Controller instances.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Counter<br>
          <strong>Frequency:</strong> Emitted every 5&nbsp;s<br>
          <strong>Applies to:</strong> cf:doppler<br>
        </td>
    </tr>
</table>

<table>
  <tr><th id="rlp-egress-dropped" colspan="2" style="text-align: center;"><br>Reverse Log Proxy Egress Dropped Messages<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>
          The Reverse Log Proxy Egress Dropped Messages indicator shows the number of messages dropped when consumers of the RLP, such as monitoring tool nozzles, ingest the exiting stream of logs and metrics too slowly. Within TAS for VMs, logs and metrics enter Loggregator for transport and then egress through the Reverse Log Proxy (RLP).
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>rlp</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>dropped</code>
        </td>
    </tr>
    <tr>
      <th>Label</th>
        <td>
          <code>direction: egress</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td><strong>Scale indicator:</strong> Scale when the rate of <code>rlp.dropped, direction: egress</code> metrics is continuously increasing.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale up the number of nozzle instances. The number of nozzle instances should match the number of Traffic Controller instances. You can scale a nozzle using the subscription ID specified when the nozzle connects to the RLP. If you use the same subscription ID on each nozzle instance, the RLP evenly distributes data across all instances of the nozzle. For example, if you have two nozzle instances with the same subscription ID, the RLP sends half of the data to one nozzle instance and half to the other. Similarly, if you have three nozzle instances with the same subscription ID, the RLP sends one-third of the data to each instance.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Counter<br>
          <strong>Frequency:</strong> Emitted every 5&nbsp;s<br>
          <strong>Applies to:</strong> cf:loggregator_trafficcontroller<br>
        </td>
    </tr>
</table>

<table>
  <tr><th id="doppler-egress-dropped" colspan="2" style="text-align: center;"><br>Doppler Egress Dropped Messages<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>
          The Doppler Egress Dropped Messages indicator shows the number of messages that the Dopplers drop when consumers of the RLP, such as monitoring tool nozzles, ingest the exiting stream of logs and metrics too slowly.
          For more information about how the Dopplers transport logs and metrics through Loggregator, see <a href="https://docs.pivotal.io/application-service/2-11/loggregator/architecture.html">Loggregator Architecture</a> in <em>Loggregator Architecture</em>.
          <p class="note"><strong>Note:</strong> The <code>doppler.dropped</code> metric includes both <code>ingress</code> and <code>egress</code> directions. To differentiate between <code>ingress</code> and <code>egress</code>, refer to the <code>direction</code> tag on the metric.</p>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>doppler</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>dropped</code><br>
          <code>egress</code>
        </td>
    </tr>
    <tr>
      <th>Label</th>
        <td>
          <code>direction: egress</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td><strong>Scale indicator:</strong> Scale when the rate of <code>doppler.dropped, direction: egress</code> metrics is continuously increasing.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale up the number of nozzle instances. The number of nozzle instances should match the number of Traffic Controller instances. You can scale a nozzle using the subscription ID specified when the nozzle connects to the RLP. If you use the same subscription ID on each nozzle instance, the RLP evenly distributes data across all instances of the nozzle. For example, if you have two nozzle instances with the same subscription ID, the RLP sends half of the data to one nozzle instance and half to the other. Similarly, if you have three nozzle instances with the same subscription ID, the RLP sends one-third of the data to each instance.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Counter<br>
          <strong>Frequency:</strong> Emitted every 5&nbsp;s<br>
          <strong>Applies to:</strong> cf:doppler<br>
        </td>
    </tr>
</table>


## <a id="scalable-syslog"></a> Syslog Drain Performance Scaling Indicators

There is a single key capacity scaling indicator <%= vars.company_name %> recommends for Syslog Drain performance.

<p class="note"><strong>Note:</strong> These Syslog Drain scaling indicators are only relevant if your deployment contains apps using the syslog drain binding feature.</p>

<table>
  <tr><th id="syslog-agent-ksi" colspan="2" style="text-align: center;"><br>Syslog Agent Loss Rate<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td>
          Divide the <code>loggregator-agent.syslog_agent.dropped{direction:egress}</code> metric by the <code>loggregator-agent.loggr-syslog-agent.ingress{scope:all_drains}</code> metric to get the rate of messages dropped as a percentage of total message traffic through Syslog Agents. The message traffic through Syslog Agents includes logs for bound apps.The loss rate of Syslog Agents indicates that the syslog drain consumer is ingesting logs from a syslog-drain-bound app too slowly.<br>
		      The Syslog Agent loss rate does not affect the Firehose loss rate. Message loss can occur in Syslog Agents without message loss occuring in the Firehose.<br>
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>loggregator-agent</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>dropped</code><br>
          <code>ingress</code>
        </td>
    </tr>
    <tr>
      <th>Label</th>
        <td>
          <code>direction:egress</code><br>
          <code>scope:all_drains</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>
          The scaling indicator <%= vars.company_name %> recommends is the minimum Syslog Agent loss rate per minute within a five-minute window. You should scale up if the loss rate is greater than <code>0.1</code> for five minutes or longer.<br><br>
          <strong>Scale indicator:</strong> &ge; 0.1</br>
          If alerting:<br>
          <strong>Yellow warning:</strong> &ge; 0.01<br>
          <strong>Red critical:</strong> &ge; 0.1</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Review the logs of the syslog server for intake issues and other performance issues. Scale the syslog server if necessary.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Counter (Integer)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
        </td>
    </tr>
</table>


## <a id="log-cache"></a> Log Cache Scaling Indicator

<%= vars.company_name %> recommends the following scaling indicator for monitoring the performance of Log Cache.

<table>
  <tr><th id="cache-duration" colspan="2" style="text-align: center;"><br>Log Cache Caching Duration<br><br></th></tr>
    <tr>
      <th width="25">Description</th>
        <td> The Log Cache Caching Duration indicator shows the age in milliseconds of the oldest data point stored in Log Cache.<br>
        Log Cache stores all messages that are passed through the Firehose in an ephemeral in-memory store. The size of this store and the cache duration are dependent on the amount of memory available on the VM where Log Cache runs. Typically, Log Cache runs on the Doppler VM. 
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>log_cache</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>log_cache_cache_period</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td><strong>Scale indicator:</strong> Scale the VM on which Log Cache runs when the <code>log_cache_cache_period</code> metric drops below 900000 milliseconds.</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale up the number of Doppler VMs or choose a VM type for Doppler that provides more memory.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge<br>
          <strong>Frequency:</strong> Emitted every 15&nbsp;s<br>
          <strong>Applies to:</strong> cf:log-cache<br>
        </td>
    </tr>
</table>


## <a id="router"></a> Gorouter Performance Scaling Indicator

There is one key capacity scaling indicator <%= vars.company_name %> recommends for Gorouter performance.

<p class= "note"><strong>Note:</strong> The following metric appears in the Firehose in two different formats. The below table lists both formats. For more information, see <a href="https://docs.pivotal.io/platform/2-8/pcf-release-notes/runtime-rn.html#duplicate-metrics">Duplicate Metrics Appear in the Firehose</a> in <em><%= vars.app_runtime_full %> v2.8 Release Notes</em>.</p>

<table>
  <tr><th id="system.cpu.user" colspan="2" style="text-align: center;"><br>Gorouter VM CPU Utilization<br><br></th></tr>
    <tr>
      <th>Description</th>
        <td>The Gorouter VM CPU Utilization indicator shows how much of a Gorouter VM's CPU is being used. 
        High CPU utilization of the Gorouter VMs can increase latency and cause requests per second to decrease.
        </td>
    </tr>
    <tr>
      <th>Source ID</th>
        <td>
          <code>cpu</code>
        </td>
    </tr>
    <tr>
      <th>Metrics</th>
        <td>
          <code>user</code>
        </td>
    </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>
          <strong>Scale indicator:</strong> &ge; 60%<br>
          If alerting:<br>
          <strong>Yellow warning:</strong> &ge; 60%<br>
          <strong>Red critical:</strong> &ge; 70%<br>
        </td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>
        Scale the Gorouters horizontally or vertically by editing the Router VM in the Resource Config pane of the TAS for VMs tile. At greater than 8 CPUs, vertical scaling is no longer beneficial for increasing throughput.
        </td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (float)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:router<br>
        </td>
    </tr>
</table>


## <a id="UAA"></a> UAA Performance Scaling Indicator

There is one key capacity scaling indicator <%= vars.company_name %> recommends for UAA performance.

<p class= "note"><strong>Note:</strong> The following metric appears in the Firehose in two different formats. The below table lists both formats. For more information, see <a href="https://docs.pivotal.io/platform/2-8/pcf-release-notes/runtime-rn.html#duplicate-metrics">Duplicate Metrics Appear in the Firehose</a> in <em><%= vars.app_runtime_full %> v2.8 Release Notes</em>.</p>

<table>
  <tr><th id="uaa-system.cpu.user" colspan="2" style="text-align: center;"><br>UAA VM CPU Utilization<br><br></th></tr>
    <tr>
      <th>Description</th>
        <td>The UAA VM CPU Utilization indicator shows how much of the UAA VM's CPU is used. High CPU utilization of the UAA VMs can cause requests per second to decrease.</td>
    </tr>
    <tr>
    <th>Source ID</th>
      <td>
        <code>cpu</code>
      </td>
  </tr>
  <tr>
    <th>Metrics</th>
      <td>
        <code>user</code>
      </td>
  </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>
          <strong>Scale indicator:</strong> &ge; 80%<br>
          If alerting:<br>
          <strong>Yellow warning:</strong> &ge; 80%<br>
          <strong>Red critical:</strong> &ge; 90%<br>
        </td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Scale UAA horizontally or vertically. To scale UAA, navigate to the <strong>Resource Config</strong> pane of the <%= vars.app_runtime_abbr %> tile and edit the number of your <strong>UAA</strong> VM instances or change the VM type to a type that utilizes more CPU cores.<br>
        </td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (float)<br>
          <strong>Frequency:</strong> Emitted every 60&nbsp;s<br>
          <strong>Applies to:</strong> cf:uaa<br>
        </td>
    </tr>
</table>


## <a id="systemdiskpersist"></a> NFS/WebDAV Backed Blobstore

There is one key capacity scaling indicator for external S3 external storage.

<p class="note"><strong>Note:</strong> This metric is only relevant if your deployment does not use an external S3 repository for
  external storage with no capacity constraints.</p>

<p class= "note"><strong>Note:</strong> The following metric appears in the Firehose in two different formats. The below table lists both formats. For more information, see <a href="https://docs.pivotal.io/platform/2-8/pcf-release-notes/runtime-rn.html#duplicate-metrics">Duplicate Metrics Appear in the Firehose</a> in <em><%= vars.app_runtime_full %> v2.8 Release Notes</em>.</p>

<table>
  <tr><th colspan="2" style="text-align: center;"><br>External S3 External Storage<br><br></th></tr>
    <tr>
      <th width="25%">Description</th>
        <td>The External S3 External Storage indicator shows the percentage of persistent disk used. 
        <em>If applicable:</em> Monitor the percentage of persistent disk used on the VM for the NFS Server job.<br>
        If you do not use an external S3 repository for external storage with no capacity constraints, you must monitor the <%= vars.app_runtime_abbr %> object store to push new app and buildpacks.</td>
    </tr>
    <tr>
    <th>Source ID</th>
      <td>
        <code>disk</code>
      </td>
  </tr>
  <tr>
    <th>Metrics</th>
      <td>
        <code>persistent.percent</code>
      </td>
  </tr>
    <tr>
      <th>Recommended thresholds</th>
        <td>&ge; 75%</td>
    </tr>
    <tr>
      <th>How to scale</th>
        <td>Give your NFS Server additional persistent disk resources. If you use an internal NFS/WebDAV backed blobstore, consider scaling the persistent disk when it reaches 75% capacity.</td>
    </tr>
    <tr>
      <th>Additional details</th>
        <td>
          <strong>Type:</strong> Gauge (%)<br>
          <strong>Applies to:</strong> cf:nfs_server<br>
        </td>
    </tr>
</table>
