---
title: Getting Started with Small Footprint TAS for VMs
owner: RelEng
---

This topic describes the Small Footprint <%= vars.app_runtime_full %> (<%= vars.app_runtime_abbr %>) tile for <%= vars.platform_name %>.

The Small Footprint <%= vars.app_runtime_abbr %> is a repackaging of the <%= vars.app_runtime_abbr %> components into a smaller deployment with fewer virtual machines (VMs). For a description of the limitations that come with a smaller deployment, see [Limitations](#limits).


## <a id='compare'></a> Differentiate Small Footprint <%= vars.app_runtime_abbr %> and <%= vars.app_runtime_abbr %>

A standard <%= vars.app_runtime_abbr %> deployment must have at least 13 VMs, but Small Footprint <%= vars.app_runtime_abbr %> requires only four.

The following image displays a comparison of the number of VMs deployed by <%= vars.app_runtime_abbr %> and Small Footprint <%= vars.app_runtime_abbr %>.

<%= image_tag("ert-vms-2.0.png", :class => "no-border", :alt => "VM comparison between TAS for VMs and Small Footprint TAS for VMs. TAS for VMs has 13 required VMs and 8 optional VMs. Small Footprint TAS for VMs, on the other hand, has 4 required VMs and 5 optional VMs. The required VMs for TAS for VMs are Cloud Controller, Cloud Controller Worker, Clock Global, Diego BBS, Diego Brain, Diego Cell, Doppler Server, Loggregator Traffic Controller, NATS, Router, Syslog Adapter, Syslog Scheduler, and UAA. The optional VMs for TAS for VMs are File Storage, HAProxy, Backup Restore Node, MySQL Monitor, MySQL Proxy, MySQL Server, TCP Router, and CredHub. The required VMs in Small Footprint TAS for VMs are Compute, Control, Database, and Router. The optional VMs for Small Footprint TAS for VMs are File Storage, HAProxy, Backup Restore Node, MySQL Monitor, and TCP Router.")%>
[//]: https://docs.google.com/drawings/d/19V1SY4BGW2T--c4Y8Jm26CkA8C-6jIHXl7gpEzYlTmY/edit?usp=sharing

### <a id='usecase'></a> Use Cases

Use Small Footprint <%= vars.app_runtime_abbr %> for smaller <%= vars.platform_name %> deployments on which you intend to host 2500 or fewer apps, as described in [Limitations](#limits). If you want to use Small Footprint <%= vars.app_runtime_abbr %> in a production environment, ensure the limitations described below are not an issue in your use case.

<p class="note"><strong>Note:</strong> Small Footprint <%= vars.app_runtime_abbr %> is compatible with <%= vars.platform_name %> service tiles.</p>

Small Footprint <%= vars.app_runtime_abbr %> is also ideal for the following use cases:

* **Proof-of-concept installations:** Deploy <%= vars.platform_name %> quickly and with a small footprint for evaluation or testing purposes.

* **Sandbox installations:** Use Small Footprint <%= vars.app_runtime_abbr %> as a <%= vars.platform_name %> operator sandbox for tasks such as testing compatibility.

* **Service tile R&D:** Test a service tile against Small Footprint <%= vars.app_runtime_abbr %> instead of a standard <%= vars.app_runtime_abbr %> deployment to increase efficiency and reduce cost.

### <a id='limits'></a> Limitations

Small Footprint <%= vars.app_runtime_abbr %> has the following limitations:

* **Number of app instances:** The tile is not designed to support large numbers of app instances. You cannot scale the number of Compute VMs beyond 10 instances in the **Resource Config** pane. Small Footprint <%= vars.app_runtime_abbr %> is designed to support 2500 or fewer apps.

* **Increasing platform capacity:** You cannot upgrade the Small Footprint <%= vars.app_runtime_abbr %> tile to the standard <%= vars.app_runtime_abbr %> tile. If you expect platform usage to increase beyond the capacity of Small Footprint <%= vars.app_runtime_abbr %>, <%= vars.company_name %> recommends using the standard <%= vars.app_runtime_abbr %> tile.

* **Management plane availability during tile upgrades:** You may not be able to perform management plane operations like deploying new apps and accessing APIs for brief periods during tile upgrades. The management plane is located on the Control VM.

* **App availability during tile upgrades:** If you require availability during your upgrades, you must scale your Compute VMs to a highly available configuration. Ensure sufficient capacity exists to move app instances between Compute VM instances during the upgrade.


## <a id='arch'></a> Architecture

You can deploy Small Footprint <%= vars.app_runtime_abbr %> with a minimum of four VMs, as shown in the image below.

<p class="note"><strong>Note:</strong> The following image assumes that you are using an external blobstore.</p>

<%= image_tag("srt-vms.png", :class => "no-border", :alt => "Small Footprint TAS for VMs has 4 required VMs and 5 optional VMs. The required VMs are Compute, Control, Database, and Router. The optional VMs are File Storage, HAProxy, Backup Restore Node, MySQL Monitor, and TCP Router.")%>
[//]: https://docs.google.com/a/pivotal.io/drawings/d/13ycJou77RzIb3DeLF1rUOdmEiokNj3_-VTkmILcr6dM/edit?usp=sharing

To reduce the number of VMs required for Small Footprint <%= vars.app_runtime_abbr %>, the Control and Database VMs include colocated jobs that run on a single VM in <%= vars.app_runtime_abbr %>. See the next sections for details.

For more information about the components mentioned on this page, see [<%= vars.app_runtime_abbr %> Components](../concepts/architecture).

### <a id='controlvm'></a> Control VM

The Control VM includes the <%= vars.app_runtime_abbr %> jobs that handle management plane operations, app lifecycles, logging, and user authorization and authentication. Additionally, all errands run on the Control VM, eliminating the need for a VM for each errand and significantly reducing the time it takes to run errands.

The following image shows all the jobs from <%= vars.app_runtime_abbr %> that are colocated on the Control VM in Small Footprint <%= vars.app_runtime_abbr %>.

<%= image_tag("control-vms-2.0.png", :class => "no-border", :alt => "Small Footprint TAS for VMs Condenses 11 TAS for VMs VMs onto 1 VM called Control. The 11 TAS for VMs VMs are Cloud Controller, Cloud Controller Worker, Clock Global, Diego BBS, Diego Brain, Doppler Server, Loggregator Traffic Controller, Syslog Adapter, Syslog Scheduler, UAA, and CredHub.")%>
[//]: https://docs.google.com/a/pivotal.io/drawings/d/17AZ5xT_K8O0cATDZARPtgTEDV2cy6OfEbZybv81gqAg/edit?usp=sharing

### <a id='dbvm'></a> Database VM

The Database VM includes the <%= vars.app_runtime_abbr %> jobs that handle internal storage and messaging.

The following image shows all the jobs from <%= vars.app_runtime_abbr %> that are colocated on the Database VM in Small Footprint <%= vars.app_runtime_abbr %>.

<%= image_tag("db-vms.png", :class => "no-border", :alt => "Small Footprint TAS for VMs Condenses 3 TAS for VMs VMs onto 1 VM called Database. The three TAS for VMs VMs are NATS, MySQL Proxy, and MySQL Server.")%>
[//]: https://docs.google.com/a/pivotal.io/drawings/d/12UkinQ58ejlq9AZQvwq6r7Ghtp4RnTopI_-gTGerpU4/edit?usp=sharing

### <a id='computevm'></a> Compute VM

The Compute VM is the same as the Diego Cell VM in <%= vars.app_runtime_abbr %>.

<%= image_tag("compute-vms.png", :class => "no-border", :alt => "Small Footprint TAS for VMs calls the Diego Cell VM the Compute VM")%>
[//]: https://docs.google.com/a/pivotal.io/drawings/d/1xOx5V1TGLoCCgIXuixTM1K9PlSTypmwHdt4xbN81BNY/edit?usp=sharing

### <a id='othervms'></a> Other VMs (Unchanged)

The following image shows the VMs performs the same functions in both versions of the <%= vars.app_runtime_abbr %> tile.

<%= image_tag("unchanged-vms.png", :class => "no-border", :alt => "The Router, File Storage, HAProxy, Backup Restore Node, MySQL Monitor, and TCP Router VMs are the same in both TAS for VMs and small footprint TAS for VMs.")%>
[//]: https://docs.google.com/a/pivotal.io/drawings/d/12vWg66BUjOsSK4qC9vMqNxO9Ea1kdzfM2cH4yEAEgsg/edit?usp=sharing


## <a id='requirements'></a> Requirements

The following topics list the minimum resources needed to run Small Footprint <%= vars.app_runtime_abbr %> on the public IaaSes that <%= vars.platform_name %> supports:

* [Installing <%= vars.platform_name %> on AWS](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/aws.html#aws)
* [Installing <%= vars.platform_name %> on Azure](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/azure.html#azure)
* [Installing <%= vars.platform_name %> on GCP](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/gcp.html#gcp)
* [Installing <%= vars.platform_name %> on vSphere](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/vsphere.html#vsphere-reqs)


## <a id='install'></a> Installing Small Footprint <%= vars.app_runtime_abbr %>

To install Small Footprint <%= vars.app_runtime_abbr %>, see [Architecture and Installation Overview](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/installing/index.html) and the installation and configuration topics for your IaaS.

Follow the same installation and configuration steps as for <%= vars.app_runtime_abbr %>, with these differences:

* **Selecting a product in <%= vars.product_network %>:** When you navigate to the [<%= vars.app_runtime_full %>](https://network.pivotal.io/products/elastic-runtime) page on <%= vars.product_network %>, select the **Small Footprint** release.

* **Configuring resources:**
	* The **Resource Config** pane in the Small Footprint <%= vars.app_runtime_abbr %> tile reflects the differences in VMs discussed in [Architecture](#arch).
	* Small Footprint <%= vars.app_runtime_abbr %> does not default to a highly available configuration like <%= vars.app_runtime_abbr %> does. It defaults to a minimum configuration. To make Small Footprint <%= vars.app_runtime_abbr %> highly available, scale the VMs to the following instance counts:
		* **Compute**: `3`
		* **Control**: `2`
		* **Database**: `3`
		* **Router**: `3`

* **Configuring load balancers:** If you are using an SSH load balancer, you must enter its name in the **Control VM** row of the **Resource Config** pane. There is no **Diego Brain** row in Small Footprint <%= vars.app_runtime_abbr %> because the Diego Brain is colocated on the Control VM. You can still enter the appropriate load balancers in the **Router** and **TCP Router** rows as normal.


## <a id='logs'></a> Troubleshooting Colocated Jobs Using Logs

To troubleshoot a job that runs on the Control or Database VMs:

1. Follow the procedures in _Advanced Troubleshooting with the BOSH CLI_ to  the log in to the BOSH Director for your deployment:
	1. [Gather Credential and IP Address Information](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/trouble-advanced.html#gather)
	1. [SSH into <%= vars.ops_manager %>](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/trouble-advanced.html#ssh)
	1. [Log in to the BOSH Director](https://docs.pivotal.io/platform/<%= vars.current_major_version.sub('.', '-') %>/customizing/trouble-advanced.html#log-in)

1. Use BOSH to list the VMs in your Small Footprint <%= vars.app_runtime_abbr %> deployment. Run:

		bosh -e BOSH-ENV -d TAS-DEPLOYMENT vms

	Where:
	* `BOSH-ENV` is the name of your BOSH environment.
	* `TAS-DEPLOYMENT` is the name of your Small Footprint <%= vars.app_runtime_abbr %> deployment.
	<p class="note"><strong>Note:</strong> If you do not know the name of your deployment, you can run <code>bosh -e BOSH-ENV deployments</code> to list the deployments for your BOSH Director.</p>

1. Use BOSH to SSH into one of the Small Footprint <%= vars.app_runtime_abbr %> VMs. Run:

		bosh -e BOSH-ENV -d TAS-DEPLOYMENT ssh VM-NAME/VM-GUID

	Where:
	* `BOSH-ENV` is the name of your BOSH environment.
	* `TAS-DEPLOYMENT` is the name of your Small Footprint <%= vars.app_runtime_abbr %> deployment.
	* `VM-NAME` is the name of your VM.
	* `VM-GUID` is the GUID of your VM.

	For example, to SSH into the Control VM, run:

		bosh -e example-env -d example-deployment ssh control/12b1b027-7ffd-43ca-9dc9-7f4ff204d86a

1. To act as a super user, run:

		sudo su

1. To list the processes running on the VM, run:

		monit summary

	The example output below lists the processes running on the Control VM. The processes listed reflect the colocation of jobs as outlined in [Architecture](#arch).
	<pre class="terminal">
	control/12b1b027-7ffd-43ca-9dc9-7f4ff204d86a:/var/vcap/bosh\_ssh/bosh\_f9d2446b18b445e# monit summary
	The Monit daemon 5.2.5 uptime: 5d 21h 10m

	Process 'bbs'                       running
	Process 'metron\_agent'              running
	Process 'locket'                    running
	Process 'route\_registrar'           running
	Process 'policy-server'             running
	Process 'silk-controller'           running
	Process 'uaa'                       running
	Process 'statsd\_injector'           running
	Process 'cloud\_controller\_ng'       running
	Process 'cloud\_controller\_worker\_local\_1' running
	Process 'cloud\_controller\_worker\_local\_2' running
	Process 'nginx\_cc'                  running
	Process 'routing-api'               running
	Process 'cloud\_controller\_clock'    running
	Process 'cloud\_controller\_worker\_1' running
	Process 'auctioneer'                running
	Process 'cc\_uploader'               running
	Process 'file\_server'               running
	Process 'nsync\_listener'            running
	Process 'ssh\_proxy'                 running
	Process 'tps\_watcher'               running
	Process 'stager'                    running
	Process 'loggregator\_trafficcontroller' running
	Process 'reverse\_log\_proxy'         running
	Process 'adapter'                   running
	Process 'doppler'                   running
	Process 'syslog\_drain\_binder'       running
	System 'system\_localhost'           running
	</pre>

1. To access logs, navigate to `/vars/vcap/sys/log` by running:

		cd /var/vcap/sys/log

1. To list the log directories for each process, run:

		ls

1. Navigate to the directory of the process that you want to view logs for. For example, for the Cloud Controller process, run:

		cd cloud_controller_ng/

	From the directory of the process, you can list and view its logs. See the following example output:
	<pre class="terminal">
	control/12b1b027-7ffd-43ca-9dc9-7f4ff204d86a:/var/vcap/sys/log/cloud\_controller\_ng# ls
	cloud\_controller\_ng\_ctl.err.log  cloud\_controller\_ng.log.2.gz  cloud\_controller\_ng.log.6.gz         drain                  pre-start.stdout.log
	cloud\_controller\_ng\_ctl.log      cloud\_controller\_ng.log.3.gz  cloud\_controller\_ng.log.7.gz         post-start.stderr.log
	cloud\_controller\_ng.log          cloud\_controller\_ng.log.4.gz  cloud\_controller\_worker\_ctl.err.log  post-start.stdout.log
	cloud\_controller\_ng.log.1.gz     cloud\_controller\_ng.log.5.gz  cloud\_controller\_worker\_ctl.log      pre-start.stderr.log
	</pre>


## <a id='rn'></a> Release Notes

The Small Footprint <%= vars.app_runtime_abbr %> tile releases alongside the <%= vars.app_runtime_abbr %> tile. For more information, see [<%= vars.app_runtime_full %> <%= vars.v_major_version %> Release Notes](https://docs.pivotal.io/platform/<%= product_info['local_product_version'].to_s.sub('.','-') %>/release-notes/runtime-rn.html).
